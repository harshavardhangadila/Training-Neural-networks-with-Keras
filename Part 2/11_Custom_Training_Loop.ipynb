{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1XLHoR0ygN7JErx2DVwcOXAvRe_NjnlBk?usp=sharing\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "O2yFKROXjn6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup and Imports\n",
        "%load_ext tensorboard\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import datetime\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "qaVcuWpEjtEO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load and Preprocess Fashion MNIST\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "X_train_full = X_train_full.astype(np.float32) / 255.0\n",
        "X_test = X_test.astype(np.float32) / 255.0\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "\n",
        "# Flatten the data\n",
        "X_train = X_train.reshape(-1, 28 * 28)\n",
        "X_valid = X_valid.reshape(-1, 28 * 28)\n",
        "X_test = X_test.reshape(-1, 28 * 28)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Uq4-m_RjtHC",
        "outputId": "23d9c668-5543-4e8b-eb18-59a6a877dea3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Build a Model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "DlEFNMBFjtKF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8WND4krxjlDt"
      },
      "outputs": [],
      "source": [
        "# 4. Setup Optimizer, Loss, and Metrics\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.Nadam(learning_rate=0.001)\n",
        "\n",
        "train_loss_metric = keras.metrics.Mean()\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_loss_metric = keras.metrics.Mean()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Prepare Data Pipeline\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(batch_size)"
      ],
      "metadata": {
        "id": "GO5P4SjfmWPR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. TensorBoard Setup\n",
        "logdir = os.path.join(\"logs\", \"custom_loop_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "writer = tf.summary.create_file_writer(logdir)\n"
      ],
      "metadata": {
        "id": "yWAli4lOmWR1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Custom Training Loop\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
        "\n",
        "    for step, (X_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True)\n",
        "            loss = loss_fn(y_batch, y_pred)\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        train_loss_metric.update_state(loss)\n",
        "        train_acc_metric.update_state(y_batch, y_pred)\n",
        "\n",
        "    train_loss = train_loss_metric.result().numpy()\n",
        "    train_acc = train_acc_metric.result().numpy()\n",
        "\n",
        "    # Validation\n",
        "    for X_batch_val, y_batch_val in valid_dataset:\n",
        "        y_val_pred = model(X_batch_val, training=False)\n",
        "        val_loss_metric.update_state(loss_fn(y_batch_val, y_val_pred))\n",
        "        val_acc_metric.update_state(y_batch_val, y_val_pred)\n",
        "\n",
        "    val_loss = val_loss_metric.result().numpy()\n",
        "    val_acc = val_acc_metric.result().numpy()\n",
        "\n",
        "    # Log to TensorBoard\n",
        "    with writer.as_default():\n",
        "        tf.summary.scalar(\"Loss/Train\", train_loss, step=epoch)\n",
        "        tf.summary.scalar(\"Accuracy/Train\", train_acc, step=epoch)\n",
        "        tf.summary.scalar(\"Loss/Val\", val_loss, step=epoch)\n",
        "        tf.summary.scalar(\"Accuracy/Val\", val_acc, step=epoch)\n",
        "\n",
        "    print(f\"  Train loss: {train_loss:.4f}, acc: {train_acc:.4f} | Val loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "# Reset metrics after each epoch\n",
        "train_loss_metric.reset_state()\n",
        "train_acc_metric.reset_state()\n",
        "val_loss_metric.reset_state()\n",
        "val_acc_metric.reset_state()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRaP6_iRmWU1",
        "outputId": "af57ba8d-3200-47a5-8359-cd430cc6e530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train loss: 0.4388, acc: 0.8449 | Val loss: 0.3813, acc: 0.8641\n",
            "Epoch 2/10\n",
            "  Train loss: 0.4057, acc: 0.8557 | Val loss: 0.3643, acc: 0.8702\n",
            "Epoch 3/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Launch TensorBoard\n",
        "print(\"TensorBoard ready below:\")\n",
        "%tensorboard --logdir=logs --port=6018"
      ],
      "metadata": {
        "id": "ectjEY8mmWXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}